version: '3.8'

services:
  # ==========================================================================
  # PostgreSQL - Unity Catalog metadata store
  # ==========================================================================
  postgres:
    image: postgres:16.4-alpine
    container_name: uc-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: unity_catalog
      POSTGRES_USER: uc_admin
      POSTGRES_PASSWORD: ${UC_DB_PASSWORD}
    volumes:
      - ./pgdata:/var/lib/postgresql/data
    # Internal network only - not exposed to LAN
    expose:
      - "5432"
    networks:
      - uc-internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U uc_admin -d unity_catalog"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================================================
  # Unity Catalog Server
  # ==========================================================================
  unity-catalog:
    image: unitycatalog/unitycatalog:v0.3.1
    container_name: unity-catalog
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      # Config (read-only until auth enabled, then :rw for token.txt)
      - ./etc/conf:/opt/unitycatalog/etc/conf:ro
      - ./etc/data:/opt/unitycatalog/etc/data
      # Delta Lake storage - mount your lake here
      - ${LAKE_PATH:-/lake}:/lake:rw
    ports:
      # SECURITY: Localhost only by default
      # Change to "<your-lan-ip>:8080:8080" only after enabling auth
      - "127.0.0.1:8080:8080"
    networks:
      - uc-internal
      - uc-external
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/api/2.1/unity-catalog/catalogs || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ==========================================================================
  # HashiCorp Vault - Secrets Management
  # ==========================================================================
  vault:
    image: hashicorp/vault:1.15
    container_name: vault
    restart: unless-stopped
    cap_add:
      - IPC_LOCK
    environment:
      VAULT_ADDR: "http://127.0.0.1:8200"
      VAULT_API_ADDR: "http://127.0.0.1:8200"
    volumes:
      - ./vault/config:/vault/config
      - ./vault/data:/vault/data
      - ./vault/logs:/vault/logs
    ports:
      # SECURITY: Localhost only by default
      - "127.0.0.1:8200:8200"
    networks:
      - uc-external
    command: vault server -config=/vault/config/vault.hcl
    healthcheck:
      test: ["CMD", "vault", "status", "-address=http://127.0.0.1:8200"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ==========================================================================
  # MinIO - S3-compatible Object Storage
  # ==========================================================================
  # CRITICAL: MINIO_DATA_PATH must point to LOCAL disks, NOT NFS-mounted storage.
  # MinIO-on-NFS has weird locking behavior and failure modes.
  minio:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: minio
    restart: unless-stopped
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      # For NAS: use local disk path like /volume1/forge-data
      # For dev: use ./minio-data (default)
      - ${MINIO_DATA_PATH:-./minio-data}:/data
    ports:
      # SECURITY: Localhost only by default
      - "127.0.0.1:9000:9000"   # S3 API
      - "127.0.0.1:9001:9001"   # Console
    networks:
      - uc-external
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ==========================================================================
  # Hive Metastore - Table Resolution for Spark
  # ==========================================================================
  # Provides "table name â†’ S3 location" lookups. Spark connects via thrift.
  # Before first run: docker exec -it uc-postgres psql -U uc_admin -c "CREATE DATABASE metastore;"
  metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./hive/lib/postgresql-42.7.3.jar:/opt/hive/lib/postgresql-42.7.3.jar:ro
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore
        -Djavax.jdo.option.ConnectionUserName=uc_admin
        -Djavax.jdo.option.ConnectionPassword=${UC_DB_PASSWORD}
      # S3/MinIO access for table location resolution
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    ports:
      - "127.0.0.1:9083:9083"
    networks:
      - uc-internal
      - uc-external
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9083 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ==========================================================================
  # MLflow Tracking Server - Model Registry (Armory)
  # ==========================================================================
  # Before first run: docker exec -it uc-postgres psql -U uc_admin -c "CREATE DATABASE mlflow;"
  mlflow:
    build: ./mlflow
    container_name: mlflow
    restart: unless-stopped
    environment:
      # Postgres backend for experiment/run metadata
      MLFLOW_BACKEND_STORE_URI: postgresql://uc_admin:${UC_DB_PASSWORD}@postgres:5432/mlflow
      # S3/MinIO for artifacts - tightened path prevents clutter in mlflow/ root
      MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://forge/mlflow/artifacts
      # MinIO credentials for artifact storage
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      # Path-style addressing for MinIO compatibility
      AWS_S3_ADDRESSING_STYLE: path
    ports:
      # Port 5002 to avoid conflict with Marquez (5000)
      - "127.0.0.1:5002:5000"
    networks:
      - uc-internal
      - uc-external
    command: mlflow server --host 0.0.0.0 --port 5000
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:5000/health')\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==========================================================================
  # Marquez - OpenLineage Backend (Lineage Plane)
  # ==========================================================================
  # Before first run: docker exec -it uc-postgres psql -U uc_admin -c "CREATE DATABASE marquez;"
  marquez:
    image: marquezproject/marquez:0.41.0
    container_name: marquez
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      MARQUEZ_CONFIG: /opt/marquez/marquez.yml
      MARQUEZ_DB_PASSWORD: ${UC_DB_PASSWORD}
    volumes:
      - ./marquez/marquez.yml:/opt/marquez/marquez.yml:ro
    ports:
      - "127.0.0.1:5000:5000"   # API
      - "127.0.0.1:5001:5001"   # Admin
    networks:
      - uc-internal
      - uc-external
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/v1/namespaces"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  marquez-web:
    image: marquezproject/marquez-web:0.41.0
    container_name: marquez-web
    restart: unless-stopped
    environment:
      MARQUEZ_HOST: marquez
      MARQUEZ_PORT: 5000
    ports:
      - "127.0.0.1:3000:3000"
    networks:
      - uc-external
    depends_on:
      - marquez

networks:
  uc-internal:
    internal: true
  uc-external:
